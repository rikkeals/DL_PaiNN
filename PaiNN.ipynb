{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a5723f",
   "metadata": {},
   "source": [
    "# Implementation of PaiNN\n",
    "Including gridsearch of model specific hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea1900",
   "metadata": {},
   "source": [
    "### Initializing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524daaf",
   "metadata": {},
   "source": [
    "This notebook requires the following Conda environment:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate painn\n",
    "```\n",
    "Then make sure that Jupyter Notebook is run on painn kernel\n",
    "\n",
    "\n",
    "The enviromemnt.yml file is found in GitHub Repisotory: Rikkeals/DL_PaiNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc54c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from pytorch_lightning import seed_everything\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457dbbe",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317ef3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[rng.permutation(len(dataset)).tolist()]\n",
    "\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f121490",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e879f4",
   "metadata": {},
   "source": [
    "Helper functions for the PaiNN model\n",
    "\n",
    "Functions: \n",
    "\n",
    "Local Edges - Defines atom's neighbors within a molecule\n",
    "\n",
    "RDF - Computes the radial distribution function (RDF) for distance between atompairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e956f",
   "metadata": {},
   "source": [
    "#### Local edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece0ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_edges(atom_positions, \n",
    "                graph_indexes,\n",
    "                cutoff_dist, \n",
    "                device):\n",
    "    \"\"\"\n",
    "    Computes the local edges for a set of atom positions and graph indexes.\n",
    "\n",
    "    Args:\n",
    "        atom_positions (torch.Tensor): Tensor of shape (N, 3) containing the positions of N atoms.\n",
    "        graph_indexes (torch.Tensor): Tensor of shape (M, 2) containing the indexes of M edges.\n",
    "        cutoff_dist (float): The cutoff distance for defining local edges.\n",
    "        device (str): The device to perform computations on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        edge_indexes (torch.Tensor): Tensor of shape (K, 2) containing the indexes of K edges that are valid for message passing.\n",
    "        edge_distances (torch.Tensor): Tensor of shape (K,) containing the distances of the K edges.\n",
    "        edge_directions (torch.Tensor): Tensor of shape (K, 3) containing the direction vectors of the K edges.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of atoms\n",
    "    num_atoms = graph_indexes.shape[0]\n",
    "\n",
    "    # Pairwise comparisson between all atom to find which ones are neighbors\n",
    "    pos_i = atom_positions.unsqueeze(0).repeat(num_atoms, 1, 1)\n",
    "    pos_j = atom_positions.unsqueeze(1).repeat(1, num_atoms, 1)\n",
    "\n",
    "    # Compute the relative positions and distances between all atom pairs\n",
    "    rel_pos_ij = (pos_j - pos_i).to(device)\n",
    "    dist_ij = torch.norm(rel_pos_ij, dim=2)\n",
    "\n",
    "    # Masks neeeded for atom pairs\n",
    "    # Within the cutoff distance\n",
    "    cutoff_mask = dist_ij <= cutoff_dist\n",
    "    # Not self-interaction\n",
    "    self_interaction_mask = torch.arange(num_atoms).unsqueeze(0) != torch.arange(num_atoms).unsqueeze(1)\n",
    "    # In same molecule\n",
    "    same_molecule_mask = graph_indexes.unsqueeze(0) == graph_indexes.unsqueeze(1)\n",
    "\n",
    "    #Make sure they are on the same device\n",
    "    cutoff_mask = cutoff_mask.to(device)\n",
    "    self_interaction_mask = self_interaction_mask.to(device)\n",
    "    same_molecule_mask = same_molecule_mask.to(device)\n",
    "\n",
    "    # Combine masks to get valid edges for message passing\n",
    "    valid_edges_mask = cutoff_mask & self_interaction_mask & same_molecule_mask\n",
    "\n",
    "    # Compute the edges needed for message passing\n",
    "    edge_indexes = valid_edges_mask.nonzero(as_tuple=False).t()\n",
    "    edge_distances = dist_ij[valid_edges_mask]\n",
    "    edge_directions = rel_pos_ij[valid_edges_mask]\n",
    "\n",
    "    # Return the edges and their properties\n",
    "    return edge_indexes, edge_distances, edge_directions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5713a",
   "metadata": {},
   "source": [
    "#### Radial Distribution Function (RDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee63f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RDF(edge_distances, \n",
    "        num_rbf_features,\n",
    "        cutoff_dist,\n",
    "        device):\n",
    "    \"\"\"\n",
    "    Computes the radial distribution function (RDF) for a set of edge distances, and \n",
    "    thereby expands distance into a learnable basis function. \n",
    "\n",
    "    Args: \n",
    "        edge_distances (torch.Tensor): Tensor of shape (K,) containing the distances of K edges.\n",
    "        num_rbf_features (int): The number of radial basis function features to compute.\n",
    "        cutoff_dist (float): The cutoff distance for defining the RDF.\n",
    "        device (str): The device to perform computations on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        edge_rdf (torch.Tensor): Tensor of shape (num_rbf_features,) containing the RDF values for the edge distances.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of local edges\n",
    "    num_edges = edge_distances.size(0)\n",
    "\n",
    "    # Create a tensor of evenly spaced RBF frequencies from 1 to 20\n",
    "    n_values = torch.linspace(1, 20, num_rbf_features, device=device)\n",
    "\n",
    "    # Expand n_values to match number of edges for element-wise RBF computation\n",
    "    n_values_expanded = n_values.unsqueeze(0).expand(num_edges, num_rbf_features)\n",
    "\n",
    "    # Expand edge distances to match n_values for broadcasting\n",
    "    edge_distances_expanded = edge_distances.unsqueeze(1).expand(num_edges, num_rbf_features)\n",
    "\n",
    "    # Compute the sinusiodal RDF values for each pair of edges\n",
    "    edge_rbf = torch.sin(n_values_expanded * torch.pi * edge_distances_expanded / cutoff_dist) / (edge_distances_expanded)\n",
    "\n",
    "    return edge_rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8c013",
   "metadata": {},
   "source": [
    "### Message and Update Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ebfe2",
   "metadata": {},
   "source": [
    "#### Message function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce8b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(nn.Module): \n",
    "    \"\"\"\n",
    "    Message function for Painn Model\n",
    "\n",
    "    Args: \n",
    "    Self (nn.Module): Inherits from nn.Module\n",
    "    num_features (int): Number of features in the input data\n",
    "    num_rbf_features (int): Number of radial basis function features\n",
    "    device (str): Device to run the model on (e.g., 'cuda' or 'cpu')\n",
    "\n",
    "    Returns:\n",
    "    dsf (torch.Tensor): Output tensor after applying the message function\n",
    "    dvf (torch.Tensor): Vector of distances between nodes\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 num_rbf_features: int,\n",
    "                 device: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.device = device\n",
    "\n",
    "        # Linear layers for scalar features (sf) for each atom\n",
    "        # and expanding to 3 times the number of features\n",
    "        self.linear_sf = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, num_features * 3),\n",
    "        )\n",
    "\n",
    "        # Linear layer for radial basis function (rbf) features\n",
    "        self.linear_rbf = nn.Linear(num_rbf_features, num_features * 3)\n",
    "        # Now both sf and rbf have the same shape: num_edges x num_features * 3, \n",
    "        # so they can be combined in message computations\n",
    "\n",
    "    def CosineCutoff(self,\n",
    "                     edge_distance, \n",
    "                     cutoff_dist):\n",
    "        \"\"\"\n",
    "        Cosine cutoff function to apply a cutoff distance to the edges.\n",
    "        Args:\n",
    "            edge_distance (torch.Tensor): Distances between the edges.\n",
    "            cutoff_dist (float): Cutoff distance for the message passing.\n",
    "\n",
    "        Returns:\n",
    "            CosCut (torch.Tensor): Cutoff function values.\n",
    "        \"\"\"\n",
    "        # Cosine cutoff function\n",
    "        CosCut = 0.5 * (1 + torch.cos(torch.pi * edge_distance / cutoff_dist))\n",
    "\n",
    "        return CosCut\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                sf, \n",
    "                vf,\n",
    "                edge_indexes,\n",
    "                edge_vector,\n",
    "                edge_distance,\n",
    "                edge_rbf,\n",
    "                cutoff_dist):\n",
    "        \"\"\"\n",
    "        Forward pass of the Message function. Computes the message for each atom based on its neighbors.\n",
    "        Args:\n",
    "            sf (torch.Tensor): Scalar features of the atoms.\n",
    "            vf (torch.Tensor): Vector features of the atoms.\n",
    "            edge_indexes (torch.Tensor): Edge indexes for the message passing.\n",
    "            edge_vector (torch.Tensor): Vector features of the edges.\n",
    "            edge_distance (torch.Tensor): Distances between the edges.\n",
    "            edge_rbf (torch.Tensor): Radial basis function features of the edges.\n",
    "            cutoff_dist (float): Cutoff distance for the message passing.\n",
    "\n",
    "        Returns:\n",
    "            dsf (torch.Tensor): Scalar features after message passing.\n",
    "            dvf (torch.Tensor): Vector features after message passing.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Number of atoms in the batch\n",
    "        num_atoms = sf.size(0)\n",
    "\n",
    "        # Make empty tensors for the outputs, dsf and dvf\n",
    "        dsf = torch.zeros(num_atoms, self.num_features).to(self.device)\n",
    "        dvf = torch.zeros(num_atoms, 3, self.num_features).to(self.device)\n",
    "\n",
    "        # Gather the scalar features (sf) and vector features (vf) of the neighbors\n",
    "        # based on the edge indexes so there is one neighbor for each edge\n",
    "        Neighbors_sf = sf[edge_indexes[1]]\n",
    "        Neighbors_vf = vf[edge_indexes[1]]\n",
    "\n",
    "        # Applying the linear layers to the neighbors' scalar features\n",
    "        phi = self.linear_sf(Neighbors_sf)\n",
    "        # Linear combination of the radial basis functions\n",
    "        edge_rbf_linear = self.linear_rbf(edge_rbf)\n",
    "\n",
    "        # Define the Cosine cutoff function\n",
    "        coscut = self.CosineCutoff(edge_distance, cutoff_dist)\n",
    "\n",
    "        # Scale the features with the cutoff function\n",
    "        W = edge_rbf_linear * coscut[..., None]\n",
    "\n",
    "        final_message = W * phi\n",
    "\n",
    "        # Split the final message into three parts: Wsf, Wvf_vf and Wvf_sf\n",
    "        Wsf, Wvf_vf, Wvf_sf = torch.split(final_message, self.num_features, dim=-1)\n",
    "\n",
    "        # Aggregate the contributions from neighboring atoms (scalar feature)\n",
    "        # to update the scalar features of each atom\n",
    "        dsf = dsf.index_add_(dim=0, index=edge_indexes[0], source=Wsf, alpha=1.0)\n",
    "\n",
    "        # Normalize edge vectors to unit length seperates direction from distance\n",
    "        # to keep the direction of the vector features\n",
    "        edge_vector = edge_vector / edge_distance[..., None]\n",
    "\n",
    "        # Total edge-wise directional update pr. feature\n",
    "        # computed by mixing the vector features of the neighbors and the edge vectors\n",
    "        # using the weights Wvf_vf and Wvf_sf\n",
    "        dvec = Wvf_vf.unsqueeze(1) * Neighbors_vf + edge_vector.unsqueeze(2) * Wvf_sf.unsqueeze(1)\n",
    "\n",
    "        # Aggregate the contributions from neighboring atoms (vector feature)\n",
    "        # to update the vector features of each atom\n",
    "        dvf = dvf.index_add_(dim=0, index=edge_indexes[0], source=dvec, alpha=1.0)\n",
    "\n",
    "        return dsf, dvf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6888945",
   "metadata": {},
   "source": [
    "#### Update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c378822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update(nn.Module):\n",
    "    \"\"\"\n",
    "    Update function for Painn Model\n",
    "\n",
    "    Args: \n",
    "    Self (nn.Module): Inherits from nn.Module\n",
    "    num_features (int): Number of features in the input data\n",
    "    device (str): Device to run the model on (e.g., 'cuda' or 'cpu')\n",
    "\n",
    "    Returns:\n",
    "    dsf (torch.Tensor): Output tensor after applying the update function\n",
    "    dvf (torch.Tensor): Vector of distances between nodes\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 device: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.device = device\n",
    "\n",
    "        # Linear layers for vector features (vf) for each atom\n",
    "        # and expanding to two times the number of features\n",
    "        self.linear_vf = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features*2, bias = False)\n",
    "        )\n",
    "\n",
    "        # Linear layers for scalar and vector features (sf and vf) for each atom\n",
    "        # and expanding to 3 times the number of features\n",
    "        self.linear_sf_vf = nn.Sequential(\n",
    "            nn.Linear(2*num_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, num_features*3)\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                dsf, \n",
    "                dvf, \n",
    "                sf, \n",
    "                vf):\n",
    "        \"\"\"\n",
    "        Forward pass of the Update function. Computes the update for each atom based on its features.\n",
    "        Args:\n",
    "            self (nn.Module): Instance of the Update class.\n",
    "            sf (torch.Tensor): Scalar features of the atoms.\n",
    "            vf (torch.Tensor): Vector features of the atoms.\n",
    "\n",
    "        Returns:\n",
    "            dsf (torch.Tensor): Updated scalar features after message passing.\n",
    "            dvf (torch.Tensor): Updated vector features after message passing.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Linear combinations of the vector features\n",
    "        vf = self.linear_vf(vf)\n",
    "\n",
    "        # Split the vector features into two parts: vf_U and vf_V\n",
    "        vf_U, vf_V = torch.split(vf, self.num_features, dim=-1)\n",
    "\n",
    "        # Compute dot product of vector V and vector U across spatial dimensions\n",
    "        dot_vf = (vf_U * vf_V).sum(dim=1)\n",
    "\n",
    "        # Compute Euclidean norm of each vector in vf_V, across spatial dimensions\n",
    "        # Epsilon = 1e-8 to avoid division by zero\n",
    "        norm_vf = torch.sqrt(torch.sum(vf_V**2, dim=1)+ 1e-8)\n",
    "\n",
    "        # Applying the linear layers to the scalar and vector features\n",
    "        vec_W = self.linear_sf_vf(torch.cat([sf, norm_vf], dim=-1))\n",
    "\n",
    "        # Split the final message into three parts: Wsf, Wvf_vf and Wvf_sf\n",
    "        Wsf, Wvf_vf, Wvf_sf = torch.split(vec_W, self.num_features, dim=-1)\n",
    "\n",
    "        # Compute the final change in scalar feature \n",
    "        dsf = Wsf + dot_vf * Wvf_vf\n",
    "\n",
    "        # Compute the final change in vector feature\n",
    "        dvf = Wvf_vf.unsqueeze(1) * vf_U\n",
    "\n",
    "        return dsf, dvf\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5e10a",
   "metadata": {},
   "source": [
    "### The PaiNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9746956",
   "metadata": {},
   "source": [
    "\n",
    "The Painn Model Implementation.\n",
    "\n",
    "Copyright (c) 2023, The University of Cambridge and the authors of the Polarizable Atom Interaction Neural Network (PaiNN) paper.\n",
    "\n",
    "All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249a8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "        num_output: int = 1,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "            num_output: Number of outputs for the model. \n",
    "\n",
    "            device: Device to run the model on either 'cuda' or 'cpu'\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the PaiNN model with the given parameters.\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.num_unique_atoms = num_unique_atoms\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "        self.device = device\n",
    "\n",
    "        # Translate the atom types to a one-hot encoding, so its not letters but numbers\n",
    "        self.atom_embedding = nn.Embedding(num_unique_atoms, num_features)\n",
    "\n",
    "        # Initialize the message and update blocks for the model.\n",
    "        self.message = nn.ModuleList()\n",
    "        self.update = nn.ModuleList()\n",
    "\n",
    "        # Loop through the number of message passing layers and create the message and update blocks.\n",
    "        # Number of layers shows how many neighbors we want to consider in the message passing.\n",
    "        for i in range(num_message_passing_layers):\n",
    "            self.message.append(\n",
    "                Message(\n",
    "                    num_features,\n",
    "                    num_rbf_features,\n",
    "                    device,\n",
    "                )\n",
    "            )\n",
    "            self.update.append(\n",
    "                Update(\n",
    "                    num_features,\n",
    "                    device,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Initialize the output layer for the model.\n",
    "        # The output layer is a linear layer that takes the final node features and outputs the predicted property.\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features//2, num_output),\n",
    "        )\n",
    "     \n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass of PaiNN. Includes the readout network highlighted in blue\n",
    "        in Figure 2 in (Schütt et al., 2021) with normal linear layers which is\n",
    "        used for predicting properties as sums of atomic contributions. The\n",
    "        post-processing and final sum is perfomed with\n",
    "        src.models.AtomwisePostProcessing.\n",
    "\n",
    "        Args:\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            atom_positions: torch.FloatTensor of size [num_nodes, 3] with\n",
    "                euclidean coordinates of each node / atom.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FloatTensor of size [num_nodes, num_outputs] with atomic\n",
    "            contributions to the overall molecular property prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Learn the atom embeddings for the input atoms.\n",
    "        sf = self.atom_embedding(atoms).to(self.device)\n",
    "        vf = torch.zeros(sf.size(0),3,sf.size(1)).to(self.device)\n",
    "\n",
    "        ##### Local neigborhood #####\n",
    "        # Get the local edges for the input atoms using the helper function.\n",
    "        edge_indexes, edge_distances, edge_directions = local_edges(\n",
    "            atom_positions,\n",
    "            graph_indexes,\n",
    "            self.cutoff_dist,\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        ###### Radial Basis Function (RBF) #####\n",
    "        # Compute the radial distribution function (RDF) for the input atoms using the helper function.\n",
    "        edge_rbf = RDF(\n",
    "            edge_distances,\n",
    "            self.num_rbf_features,\n",
    "            self.cutoff_dist,\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # Move the tensors to the appropriate device\n",
    "        edge_indexes = edge_indexes.to(self.device)\n",
    "        edge_distances = edge_distances.to(self.device)\n",
    "        edge_directions = edge_directions.to(self.device)\n",
    "        edge_rbf = edge_rbf.to(self.device)\n",
    "\n",
    "        ##### Message and Update #####\n",
    "        # Loop through the number of message passing layers and perform message passing and update steps.\n",
    "        for i in range(self.num_message_passing_layers):\n",
    "            # Message passing step\n",
    "            dsf, dvf = self.message[i](\n",
    "                sf,\n",
    "                vf,\n",
    "                edge_indexes,\n",
    "                edge_directions,\n",
    "                edge_distances,\n",
    "                edge_rbf,\n",
    "                self.cutoff_dist\n",
    "            )\n",
    "\n",
    "            sf = sf + dsf\n",
    "            vf = vf + dvf\n",
    "\n",
    "            # Update step\n",
    "            sf, vf = self.update[i](\n",
    "                dsf,\n",
    "                dvf,\n",
    "                sf,\n",
    "                vf\n",
    "            )\n",
    "\n",
    "            sf = sf + dsf\n",
    "            vf = vf + dvf\n",
    "\n",
    "        ##### Output #####\n",
    "        # Compute the output for the model using the final node features.\n",
    "        atomic_contributions = self.output(sf)\n",
    "\n",
    "        return atomic_contributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ca72a",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9d231",
   "metadata": {},
   "source": [
    " Post-processing for (QM9) properties that are predicted as sums of atomic contributions. \n",
    " \n",
    " Can handle cases where atomic references are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9e0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomwisePostProcessing(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor = None,  # <- allow it to be None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            mean: Mean value to shift atomwise contributions by.\n",
    "            std: Standard deviation to scale atomwise contributions by.\n",
    "            atom_refs: (Optional) Atomic reference values. If None, skip this correction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "\n",
    "        if atom_refs is not None:\n",
    "            self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "        else:\n",
    "            self.atom_refs = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: [num_nodes, num_outputs] each atom's contribution.\n",
    "            atoms: [num_nodes] atom type of each node.\n",
    "            graph_indexes: [num_nodes] graph index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            [num_graphs, num_outputs] predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions * self.scale + self.shift\n",
    "\n",
    "        if self.atom_refs is not None:\n",
    "            atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum atomic contributions into per-graph output\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901a3d9",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3339888",
   "metadata": {},
   "source": [
    "Train the PaiNN model on QM9 dataset.\n",
    "This script prepares the data, creates the model, trains it, evaluates it, and saves the results.\n",
    "\n",
    "The data is saved in a new folder inside working directory.\n",
    "\n",
    "It saves the model, training summary, training loss pr epoch and per-molecule errors, inside runs folder that also is created in same directionary where the Jupyter File is saved. Each new model gets its own folder in 'runs', starting with the date and time the model is run, and then the specific model hyperparamters for that model. \n",
    "\n",
    "Define the appropiate parameters in the beginning of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa6dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:56:58] Using device: cpu\n",
      "[21:56:58] Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.20s/it, Train loss: 1.117e+01]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Define Arguments ---\n",
    "args = Namespace(\n",
    "    seed=0,\n",
    "    target=7,\n",
    "    data_dir='data/',\n",
    "    batch_size_train=100,\n",
    "    batch_size_inference=1000,\n",
    "    num_workers=0,\n",
    "    splits=[110000, 10000, 10831],\n",
    "    subset_size=None,\n",
    "    num_message_passing_layers=1,\n",
    "    num_features=128,\n",
    "    num_outputs=1,\n",
    "    num_rbf_features=20,\n",
    "    num_unique_atoms=100,\n",
    "    cutoff_dist=5.0,\n",
    "    lr=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_epochs=100,\n",
    ")\n",
    "\n",
    "# --- Step 2: Main Function Logic ---\n",
    "t_start = time.time()\n",
    "run_timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Using device: {device}\")\n",
    "\n",
    "# Prepare data\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Preparing data...\")\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(remove_atom_refs=True, divide_by_atoms=True)\n",
    "\n",
    "# Create model\n",
    "painn = PaiNN(\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs,\n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ")\n",
    "post_processing = AtomwisePostProcessing(args.num_outputs, y_mean, y_std, atom_refs)\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(painn.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# Create run folder\n",
    "runs_dir = \"runs\"\n",
    "experiment_params = {\n",
    "    \"target\": args.target,\n",
    "    \"layers\": args.num_message_passing_layers,\n",
    "    \"lr\": args.lr,\n",
    "    \"features\": args.num_features,\n",
    "    \"rbf\": args.num_rbf_features,\n",
    "    \"cutoff\": args.cutoff_dist,\n",
    "}\n",
    "variable_name = \"_\".join([f\"{k}_{v}\" for k, v in experiment_params.items()])\n",
    "run_folder = os.path.join(runs_dir, f\"{run_timestamp}_{variable_name}\")\n",
    "os.makedirs(run_folder, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "train_losses_per_epoch = []\n",
    "painn.train()\n",
    "pbar = trange(args.num_epochs)\n",
    "for epoch in pbar:\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    train_losses_per_epoch.append(loss_epoch)\n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "# --- Step 3: Evaluate and Save ---\n",
    "painn.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        batch = batch.to(device)\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(batch.y.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "per_sample_abs_errors = torch.abs(all_preds.squeeze() - all_targets.squeeze())\n",
    "\n",
    "# Final MAE\n",
    "mae = per_sample_abs_errors.mean()\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "final_mae = float(unit_conversion(mae))\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Test MAE: {final_mae:.3f}\")\n",
    "\n",
    "# Save model\n",
    "model_save_path = os.path.join(run_folder, \"trained_painn_model.pt\")\n",
    "torch.save(painn.state_dict(), model_save_path)\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Model saved to {model_save_path}\")\n",
    "\n",
    "# Save train loss per epoch\n",
    "with open(os.path.join(run_folder, \"train_loss_per_epoch.json\"), 'w') as f:\n",
    "    json.dump(train_losses_per_epoch, f, indent=4)\n",
    "\n",
    "# Save training summary\n",
    "model_size_mb = os.path.getsize(model_save_path) / 1e6\n",
    "training_info = {\n",
    "    \"Test_MAE\": final_mae,\n",
    "    \"Total_time_seconds\": round(time.time() - t_start, 2),\n",
    "    \"Best_train_loss\": min(train_losses_per_epoch),\n",
    "    \"Best_epoch\": train_losses_per_epoch.index(min(train_losses_per_epoch)),\n",
    "    \"Model_size_MB\": round(model_size_mb, 2),\n",
    "    \"Data_dir\": args.data_dir,\n",
    "    \"Subset_size\": args.subset_size,\n",
    "    \"Splits\": args.splits,\n",
    "    \"Num_epochs\": args.num_epochs,\n",
    "    \"Batch_size_train\": args.batch_size_train,\n",
    "    \"Batch_size_inference\": args.batch_size_inference,\n",
    "    \"Learning_rate\": args.lr,\n",
    "    \"Weight_decay\": args.weight_decay,\n",
    "    \"Num_message_passing_layers\": args.num_message_passing_layers,\n",
    "    \"Num_features\": args.num_features,\n",
    "    \"Num_outputs\": args.num_outputs,\n",
    "    \"Num_rbf_features\": args.num_rbf_features,\n",
    "    \"Num_unique_atoms\": args.num_unique_atoms,\n",
    "    \"Cutoff_distance\": args.cutoff_dist,\n",
    "    \"Target\": args.target,\n",
    "    \"Target_name\": dm.target_types[args.target],\n",
    "}\n",
    "with open(os.path.join(run_folder, \"training_summary.json\"), 'w') as f:\n",
    "    json.dump(training_info, f, indent=4)\n",
    "\n",
    "# Save per-molecule errors\n",
    "per_molecule_errors = {\n",
    "    \"molecule_indices\": list(range(len(per_sample_abs_errors))),\n",
    "    \"abs_errors\": per_sample_abs_errors.tolist(),\n",
    "}\n",
    "with open(os.path.join(run_folder, \"per_molecule_errors.json\"), 'w') as f:\n",
    "    json.dump(per_molecule_errors, f, indent=4)\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M')}] Total script time: {time.time() - t_start:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
